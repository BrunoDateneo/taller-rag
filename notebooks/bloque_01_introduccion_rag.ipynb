{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bloque 1 · ¿Qué es Retrieval-Augmented Generation (RAG)?\n",
        "\n",
        "Este bloque introduce los conceptos esenciales de un flujo RAG y cómo cada componente contribuye a obtener respuestas mejor informadas a partir de fuentes propias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ¿Por qué RAG?\n",
        "\n",
        "Los modelos de lenguaje grandes (LLM) poseen conocimiento general, pero no siempre están actualizados ni conocen los detalles internos de una organización. Retrieval-Augmented Generation (RAG) permite enriquecer las respuestas de un modelo con información específica proveniente de una base de conocimiento curada.\n",
        "\n",
        "En otras palabras, RAG combina dos mundos:\n",
        "- **Recuperación**: localizar fragmentos relevantes en documentos propios.\n",
        "- **Generación**: redactar una respuesta usando tanto la pregunta como los fragmentos recuperados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Componentes de un sistema RAG\n",
        "\n",
        "1. **Fuentes de conocimiento**: documentos, páginas web, manuales o bases de datos que contienen la información confiable.\n",
        "2. **Ingesta y limpieza**: procesos que normalizan, eliminan ruido y preparan el texto.\n",
        "3. **Chunking**: división en fragmentos manejables (chunks) que capturen ideas completas.\n",
        "4. **Modelos de embeddings**: convierten cada fragmento en un vector numérico que preserva significado.\n",
        "5. **Base vectorial**: almacena los vectores y permite realizar búsquedas por similitud.\n",
        "6. **Recuperador**: dada una consulta, encuentra los vectores más cercanos.\n",
        "7. **Generador**: redacta la respuesta final usando la consulta y los fragmentos recuperados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flujo típico de extremo a extremo\n",
        "\n",
        "1. Se seleccionan y preparan las fuentes confiables.\n",
        "2. Se crean fragmentos y sus embeddings.\n",
        "3. Se almacenan en una base vectorial.\n",
        "4. Una pregunta del usuario se transforma en vector.\n",
        "5. Se recuperan los fragmentos más similares.\n",
        "6. Un modelo genera una respuesta apoyándose en esos fragmentos.\n",
        "\n",
        "En este taller simularemos los pasos 1 a 5 y omitiremos la generación automática para que podamos observar el efecto de la recuperación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actividad práctica: simulación de vector search\n",
        "\n",
        "Trabajaremos con un conjunto reducido de fragmentos y construiremos manualmente una representación vectorial sencilla para entender cómo cambia la recuperación cuando variamos la pregunta.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 1. Definir una base de conocimiento\n",
        "\n",
        "Imaginemos que recopilamos notas internas sobre cómo implementar un sistema RAG. Cada elemento representa un párrafo breve que podremos fragmentar más adelante.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "corpus = [\n",
        "    {\n",
        "        \"id\": \"doc1\",\n",
        "        \"titulo\": \"Arquitectura general\",\n",
        "        \"contenido\": (\n",
        "            \"Un sistema RAG combina recuperación de información y modelos generativos. \"\n",
        "            \"La clave es aportar contexto actualizado y específico mediante documentos propios.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc2\",\n",
        "        \"titulo\": \"Preparación de datos\",\n",
        "        \"contenido\": (\n",
        "            \"Antes de indexar documentos conviene limpiar el texto, eliminar duplicados \"\n",
        "            \"y segmentar en fragmentos que mantengan sentido completo.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc3\",\n",
        "        \"titulo\": \"Embeddings\",\n",
        "        \"contenido\": (\n",
        "            \"Los embeddings convierten texto en vectores. \"\n",
        "            \"Dos fragmentos semánticamente similares tendrán vectores cercanos en el espacio.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc4\",\n",
        "        \"titulo\": \"Vector store\",\n",
        "        \"contenido\": (\n",
        "            \"Una base vectorial almacena los embeddings y ofrece métricas de similitud. \"\n",
        "            \"Es importante elegir índices que escalen con la cantidad de documentos.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc5\",\n",
        "        \"titulo\": \"Buenas prácticas\",\n",
        "        \"contenido\": (\n",
        "            \"La calidad de un RAG depende de la curación de las fuentes, la estrategia de chunking \"\n",
        "            \"y el ajuste de la consulta. Medir relevancia ayuda a iterar.\"\n",
        "        ),\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"Documentos disponibles: {len(corpus)}\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2. Fragmentar y tokenizar\n",
        "\n",
        "Dividiremos cada documento en frases cortas (chunks) y convertiremos el texto en tokens sencillos. Para mantener el enfoque introductorio, utilizaremos una tokenización muy básica basada en separar por espacios.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sentence_chunks(text):\n",
        "    delimiters = \".!?\"\n",
        "    current = \"\"\n",
        "    for char in text:\n",
        "        current += char\n",
        "        if char in delimiters:\n",
        "            chunk = current.strip()\n",
        "            if chunk:\n",
        "                yield chunk\n",
        "            current = \"\"\n",
        "    if current.strip():\n",
        "        yield current.strip()\n",
        "\n",
        "def tokenize(text):\n",
        "    table = str.maketrans({c: \" \" for c in \",.;:¡!¿?\\\"'\"})\n",
        "    cleaned = text.lower().translate(table)\n",
        "    return [token for token in cleaned.split() if token]\n",
        "\n",
        "chunks = []\n",
        "for doc in corpus:\n",
        "    for idx, fragment in enumerate(sentence_chunks(doc[\"contenido\"])):\n",
        "        tokens = tokenize(fragment)\n",
        "        chunks.append(\n",
        "            {\n",
        "                \"doc_id\": doc[\"id\"],\n",
        "                \"titulo\": doc[\"titulo\"],\n",
        "                \"fragmento\": fragment,\n",
        "                \"tokens\": tokens,\n",
        "            }\n",
        "        )\n",
        "\n",
        "print(f\"Chunks generados: {len(chunks)}\")\n",
        "print(\n",
        "    f\"Primer chunk de ejemplo:\\n- Documento: {chunks[0]['doc_id']} ({chunks[0]['titulo']})\\n- Texto: {chunks[0]['fragmento']}\"\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 3. Construir un espacio vectorial simple\n",
        "\n",
        "Crearemos un vocabulario con todas las palabras observadas y representaremos cada chunk como un vector de frecuencias normalizadas (TF). Aunque en producción usaríamos modelos más avanzados, este enfoque permite visualizar el mecanismo de la similitud coseno.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vocabulary = sorted({token for chunk in chunks for token in chunk[\"tokens\"]})\n",
        "token_index = {token: idx for idx, token in enumerate(vocabulary)}\n",
        "\n",
        "def vectorize(tokens):\n",
        "    if not tokens:\n",
        "        return np.zeros(len(vocabulary), dtype=float)\n",
        "    filtered = [token for token in tokens if token in token_index]\n",
        "    if not filtered:\n",
        "        return np.zeros(len(vocabulary), dtype=float)\n",
        "    counts = Counter(filtered)\n",
        "    total = sum(counts.values())\n",
        "    vector = np.zeros(len(vocabulary), dtype=float)\n",
        "    for token, freq in counts.items():\n",
        "        idx = token_index[token]\n",
        "        vector[idx] = freq / total\n",
        "    return vector\n",
        "\n",
        "chunk_vectors = np.vstack([vectorize(chunk[\"tokens\"]) for chunk in chunks])\n",
        "\n",
        "print(f\"Tamaño del vocabulario: {len(vocabulary)} términos\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 4. Recuperar fragmentos por similitud\n",
        "\n",
        "Compararemos la representación vectorial de la pregunta con cada chunk y ordenaremos por similitud coseno. Esto simula el comportamiento de una base vectorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def cosine_similarity(vector_a, vector_b):\n",
        "    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(vector_a, vector_b) / denom)\n",
        "\n",
        "def retrieve(query, top_k=3):\n",
        "    query_tokens = tokenize(query)\n",
        "    query_vector = vectorize(query_tokens)\n",
        "    scores = [cosine_similarity(query_vector, chunk_vectors[idx]) for idx in range(len(chunks))]\n",
        "    ranked = sorted(\n",
        "        zip(range(len(chunks)), scores), key=lambda item: item[1], reverse=True\n",
        "    )\n",
        "    top_hits = []\n",
        "    for idx, score in ranked[:top_k]:\n",
        "        chunk = chunks[idx]\n",
        "        top_hits.append(\n",
        "            {\n",
        "                \"similitud\": score,\n",
        "                \"fragmento\": chunk[\"fragmento\"],\n",
        "                \"titulo\": chunk[\"titulo\"],\n",
        "                \"doc_id\": chunk[\"doc_id\"],\n",
        "            }\n",
        "        )\n",
        "    return top_hits\n",
        "\n",
        "def mostrar_resultados(pregunta, top_k=3):\n",
        "    print(f\"Pregunta: {pregunta}\\n\")\n",
        "    resultados = retrieve(pregunta, top_k=top_k)\n",
        "    for pos, hit in enumerate(resultados, start=1):\n",
        "        porcentaje = round(hit[\"similitud\"] * 100, 2)\n",
        "        print(f\"#{pos} · similitud: {porcentaje}%\")\n",
        "        print(f\"Documento: {hit['doc_id']} · {hit['titulo']}\")\n",
        "        print(f\"Fragmento: {hit['fragmento']}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "ejemplo = \"¿Para qué sirve la base vectorial?\"\n",
        "mostrar_resultados(ejemplo)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experimenta cambiando la pregunta\n",
        "\n",
        "Ejecuta la celda siguiente y modifica el texto de la pregunta para observar cómo cambian los fragmentos recuperados. Prueba preguntas sobre \"embeddings\", \"preparación\", \"buenas prácticas\" u otros temas mencionados en la base de conocimiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "otra_pregunta = \"¿Qué pasos recomiendan para preparar los datos?\"\n",
        "mostrar_resultados(otra_pregunta)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflexión\n",
        "\n",
        "- Los resultados dependen de las palabras que comparten la pregunta y los fragmentos.\n",
        "- En sistemas reales usaríamos embeddings semánticos más robustos y normalizaciones adicionales.\n",
        "- Una vez recuperados los fragmentos, el siguiente paso sería pasarlos a un modelo generativo para redactar una respuesta final.\n",
        "\n",
        "En el siguiente bloque profundizaremos en cómo mejorar la representación vectorial y evaluaremos la calidad de la recuperación.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}