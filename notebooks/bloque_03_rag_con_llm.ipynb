{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bloque 3 · Combinando recuperación y generación\n",
        "\n",
        "En este bloque daremos el siguiente paso del flujo RAG: pasaremos de recuperar fragmentos (Bloque 2) a generar respuestas completas usando la API de RAG (`retrieve_and_generate`) de Amazon Bedrock. Analizaremos cómo el modelo genera texto a partir del contexto recuperado y practicaremos la personalización del *prompt*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ¿En qué se diferencia del Bloque 2?\n",
        "\n",
        "- **Bloque 2**: nos enfocamos en recuperar fragmentos relevantes mediante el endpoint `Retrieve` y aprendimos a parsear la respuesta.\n",
        "- **Bloque 3**: incorporamos un modelo generativo. Pedimos a Bedrock que, además de recuperar contextos, redacte una respuesta siguiendo nuestras instrucciones.\n",
        "\n",
        "El LLM aporta redacción fluida, síntesis y la capacidad de adaptar el tono al público. Nuestro trabajo consiste en suministrarle el contexto correcto y un *prompt* claro para reducir al mínimo las alucinaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rol del LLM dentro de un RAG\n",
        "\n",
        "- **Interpreta la pregunta** y detecta la intención.\n",
        "- **Lee los fragmentos recuperados** para extraer evidencia.\n",
        "- **Redacta** una respuesta final, aplicando el tono y formato especificado.\n",
        "- **Cita fuentes** si se lo pedimos explícitamente en el prompt.\n",
        "\n",
        "Sin un buen prompt, el LLM puede ignorar información relevante o inventar detalles. Por eso es fundamental controlar el mensaje que le enviamos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API `retrieve_and_generate`\n",
        "\n",
        "- Combina en una sola llamada la búsqueda en la knowledge base y la generación de texto.\n",
        "- Permite indicar qué modelo usar (`modelArn`), cómo recuperar los fragmentos y cómo estructurar la generación (parámetros y *prompt template*).\n",
        "- Devuelve tanto la respuesta generada como los fragmentos utilizados, para auditar y mostrar citas.\n",
        "\n",
        "Trabajaremos directamente con esta API usando `boto3`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diseño del prompt\n",
        "\n",
        "Un *prompt template* típico para RAG incluye:\n",
        "- Un mensaje al modelo sobre su rol (`Eres un asistente experto en ...`).\n",
        "- Las instrucciones sobre cómo usar el contexto (`Solo responde usando la información proporcionada`).\n",
        "- El espacio reservado para insertar los fragmentos (`{context}`) y la pregunta (`{query}`).\n",
        "- Indicaciones de formato y tono.\n",
        "\n",
        "La actividad práctica se centrará en modificar este template para observar cómo cambia la respuesta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparación antes de ejecutar\n",
        "\n",
        "1. Ejecuta la celda de instalación de dependencias del **Bloque 0**.\n",
        "2. Define las siguientes variables de entorno:\n",
        "   - `BEDROCK_KB_ID`: identificador de la knowledge base.\n",
        "   - `BEDROCK_MODEL_ARN`: ARN del modelo generativo disponible en tu cuenta (por ejemplo, Titan Text G1 o Claude 3 en Bedrock).\n",
        "   - `BEDROCK_DATA_SOURCE_ID`: identificador del data source asociado a la knowledge base (necesario para lanzar ingestas).\n",
        "   - `AWS_REGION`: solo si trabajas fuera de `us-east-1`.\n",
        "3. Asegúrate de haber subido a S3 los documentos que quieras ingerir y de que el data source tenga acceso a esa ruta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import time\n",
        "from typing import Any, Dict\n",
        "\n",
        "import boto3\n",
        "\n",
        "AWS_REGION = os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
        "KNOWLEDGE_BASE_ID = os.environ.get(\"BEDROCK_KB_ID\")\n",
        "MODEL_ARN = os.environ.get(\"BEDROCK_MODEL_ARN\")\n",
        "DATA_SOURCE_ID = os.environ.get(\"BEDROCK_DATA_SOURCE_ID\")\n",
        "\n",
        "if not KNOWLEDGE_BASE_ID:\n",
        "    raise ValueError(\"Falta la variable de entorno BEDROCK_KB_ID.\")\n",
        "if not MODEL_ARN:\n",
        "    raise ValueError(\"Falta la variable de entorno BEDROCK_MODEL_ARN.\")\n",
        "\n",
        "rag_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=AWS_REGION)\n",
        "rag_admin = boto3.client(\"bedrock-agent\", region_name=AWS_REGION)\n",
        "\n",
        "print(\"Clientes de Bedrock inicializados correctamente.\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actividad 1 · Ajustar el prompt y analizar la respuesta\n",
        "\n",
        "1. Modifica el template para cambiar el tono, la estructura o los requisitos (por ejemplo, añadir una lista de pasos, citar la fuente exacta, responder en otro idioma, etc.).\n",
        "2. Ejecuta la celda para generar una respuesta.\n",
        "3. Observa cómo el modelo sigue (o no) tus instrucciones y revisa las citas devueltas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DEFAULT_PROMPT_TEMPLATE = \"\"\"\n",
        "Eres un asistente especializado en la plataforma interna de la empresa.\n",
        "Responde a la pregunta usando únicamente la información de CONTEXTO.\n",
        "Si la respuesta no está en el contexto, admite que no puedes responder.\n",
        "\n",
        "CONTEXTO:\n",
        "{context}\n",
        "\n",
        "PREGUNTA:\n",
        "{query}\n",
        "\n",
        "Respuesta estructurada en párrafos cortos y menciona la fuente si está disponible.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def generar_con_prompt(\n",
        "    pregunta: str,\n",
        "    prompt_template: str,\n",
        "    top_k: int = 4,\n",
        "    max_tokens: int = 600,\n",
        "    temperature: float = 0.2,\n",
        "    top_p: float = 0.9,\n",
        ") -> Dict[str, Any]:\n",
        "    configuracion = {\n",
        "        \"type\": \"KNOWLEDGE_BASE\",\n",
        "        \"knowledgeBaseConfiguration\": {\n",
        "            \"knowledgeBaseId\": KNOWLEDGE_BASE_ID,\n",
        "            \"modelArn\": MODEL_ARN,\n",
        "            \"generationConfiguration\": {\n",
        "                \"promptTemplate\": {\n",
        "                    \"textPromptTemplate\": prompt_template,\n",
        "                },\n",
        "                \"temperature\": temperature,\n",
        "                \"topP\": top_p,\n",
        "                \"maxTokens\": max_tokens,\n",
        "            },\n",
        "            \"retrievalConfiguration\": {\n",
        "                \"vectorSearchConfiguration\": {\n",
        "                    \"numberOfResults\": top_k,\n",
        "                }\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    respuesta = rag_runtime.retrieve_and_generate(\n",
        "        input={\"text\": pregunta},\n",
        "        retrieveAndGenerateConfiguration=configuracion,\n",
        "    )\n",
        "    return respuesta\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from textwrap import wrap\n",
        "\n",
        "\n",
        "def mostrar_generacion(respuesta: Dict[str, Any]) -> None:\n",
        "    output = respuesta.get(\"output\", {})\n",
        "    texto = output.get(\"text\", \"<Sin respuesta>\")\n",
        "\n",
        "    print(\"Respuesta generada:\\n\")\n",
        "    for linea in wrap(texto, width=100):\n",
        "        print(linea)\n",
        "\n",
        "    print(\"\\nCitas encontradas:\")\n",
        "    citas = respuesta.get(\"citations\", [])\n",
        "    if not citas:\n",
        "        print(\"- No se devolvieron citas. Revisa el prompt o la coverage de la knowledge base.\")\n",
        "    else:\n",
        "        for idx, cita in enumerate(citas, start=1):\n",
        "            referencia = cita.get(\"reference\", {})\n",
        "            fuente = referencia.get(\"location\", {}).get(\"s3Location\", {}).get(\"uri\", \"Fuente desconocida\")\n",
        "            print(f\"- Cita #{idx}: {fuente}\")\n",
        "\n",
        "\n",
        "pregunta_inicial = \"¿Cuáles son las mejores prácticas para mantener actualizada la knowledge base de RAG?\"\n",
        "prompt_personalizado = DEFAULT_PROMPT_TEMPLATE\n",
        "\n",
        "respuesta_generada = generar_con_prompt(pregunta_inicial, prompt_personalizado)\n",
        "mostrar_generacion(respuesta_generada)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experimenta cambiando `prompt_personalizado` y vuelve a ejecutar la celda anterior. Observa cómo el tono, la longitud y la presencia de citas dependen de tu template y de los parámetros de generación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Actividad 2 · Añadir nuevos documentos y repetir la pregunta\n",
        "\n",
        "Objetivo: demostrar cómo un flujo RAG puede incorporar conocimiento reciente sin reentrenar el modelo.\n",
        "\n",
        "1. Formula una pregunta sobre un documento **que todavía no esté** en la knowledge base.\n",
        "2. Observa cómo responde el sistema (probablemente indicando que no tiene contexto suficiente).\n",
        "3. Añade el documento al data source y lanza una nueva ingesta.\n",
        "4. Vuelve a hacer la pregunta y verifica cómo ahora el LLM puede responder basándose en la nueva información."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pregunta_nueva = \"¿Qué pasos debemos seguir para desplegar el chatbot en el entorno de pruebas?\"\n",
        "prompt_para_nuevo_doc = DEFAULT_PROMPT_TEMPLATE\n",
        "\n",
        "respuesta_sin_doc = generar_con_prompt(pregunta_nueva, prompt_para_nuevo_doc)\n",
        "mostrar_generacion(respuesta_sin_doc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si la knowledge base todavía no contiene el documento, la respuesta debería dejar en claro que no existe suficiente contexto. Ahora lanzaremos una ingesta para incorporar la nueva información."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lanzar una nueva ingesta desde código\n",
        "\n",
        "- Sube el documento (por ejemplo, `procedimiento_chatbot.pdf`) al bucket configurado en tu data source.\n",
        "- Confirma que `BEDROCK_DATA_SOURCE_ID` tiene el identificador correcto.\n",
        "- Ejecuta la siguiente celda para iniciar el job de ingesta y espera a que finalice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if not DATA_SOURCE_ID:\n",
        "    raise ValueError(\"Define la variable BEDROCK_DATA_SOURCE_ID para lanzar ingestas desde código.\")\n",
        "\n",
        "ingestion_job = rag_admin.start_ingestion_job(\n",
        "    knowledgeBaseId=KNOWLEDGE_BASE_ID,\n",
        "    dataSourceId=DATA_SOURCE_ID,\n",
        ")\n",
        "\n",
        "job_id = ingestion_job[\"ingestionJob\"][\"ingestionJobId\"]\n",
        "print(f\"Job de ingesta iniciado: {job_id}\")\n",
        "\n",
        "estado_final = None\n",
        "while True:\n",
        "    detalle = rag_admin.get_ingestion_job(\n",
        "        knowledgeBaseId=KNOWLEDGE_BASE_ID,\n",
        "        dataSourceId=DATA_SOURCE_ID,\n",
        "        ingestionJobId=job_id,\n",
        "    )\n",
        "    estado = detalle[\"ingestionJob\"][\"status\"]\n",
        "    print(f\"Estado actual: {estado}\")\n",
        "    if estado in {\"COMPLETE\", \"FAILED\", \"STOPPED\"}:\n",
        "        estado_final = estado\n",
        "        break\n",
        "    time.sleep(15)\n",
        "\n",
        "if estado_final != \"COMPLETE\":\n",
        "    raise RuntimeError(f\"La ingesta no finalizó correctamente (estado: {estado_final}).\")\n",
        "\n",
        "print(\"La knowledge base se actualizó con la nueva información.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez completada la ingesta, vuelve a ejecutar la consulta con la misma pregunta para comprobar la diferencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "respuesta_con_doc = generar_con_prompt(pregunta_nueva, prompt_para_nuevo_doc)\n",
        "mostrar_generacion(respuesta_con_doc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflexión final\n",
        "\n",
        "- Un *prompt* bien redactado guía al modelo para usar únicamente la evidencia recuperada.\n",
        "- La API de RAG permite iterar rápidamente: añadir documentos y volver a preguntar sin reentrenar el modelo.\n",
        "- La trazabilidad (citas) es clave para generar confianza en el chatbot.\n",
        "\n",
        "En el siguiente bloque integraremos todo esto dentro de Chainlit para ofrecer una experiencia conversacional completa."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}