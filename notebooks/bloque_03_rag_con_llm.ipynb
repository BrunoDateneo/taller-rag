{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "AiauItenP17E",
      "metadata": {
        "id": "AiauItenP17E"
      },
      "source": [
        "# Bloque 3 · Combinando recuperación y generación\n",
        "\n",
        "En este bloque daremos el siguiente paso del flujo RAG: pasaremos de recuperar fragmentos (Bloque 2) a generar respuestas completas usando la API de RAG (`retrieve_and_generate`) de Amazon Bedrock. Analizaremos cómo el modelo genera texto a partir del contexto recuperado y practicaremos la personalización del *prompt*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CR19WKQSP17F",
      "metadata": {
        "id": "CR19WKQSP17F"
      },
      "source": [
        "## ¿En qué se diferencia del Bloque 2?\n",
        "\n",
        "- **Bloque 2**: nos enfocamos en recuperar fragmentos relevantes mediante el endpoint `Retrieve` y aprendimos a parsear la respuesta.\n",
        "- **Bloque 3**: incorporamos un modelo generativo. Pedimos a Bedrock que, además de recuperar contextos, redacte una respuesta siguiendo nuestras instrucciones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472fb240",
      "metadata": {
        "id": "472fb240"
      },
      "source": [
        "## LLM\n",
        "\n",
        "### ¿Qué es un LLM?\n",
        "\n",
        "Un **LLM (Large Language Model, Modelo de Lenguaje Grande)** es un tipo de modelo de inteligencia artificial entrenado con grandes volúmenes de texto para entender y generar lenguaje natural. Estos modelos aprenden patrones, relaciones semánticas y estructuras del lenguaje humano a partir de millones o billones de documentos.\n",
        "\n",
        "Los LLMs pueden:\n",
        "- **Comprender** texto en lenguaje natural (preguntas, instrucciones, contexto)\n",
        "- **Generar** texto coherente y contextualmente relevante\n",
        "- **Sintetizar** información de múltiples fuentes\n",
        "- **Adaptar** el tono y estilo según las instrucciones recibidas\n",
        "\n",
        "Ejemplos populares incluyen modelos como Claude (de Anthropic), GPT (de OpenAI), y Titan Text (de AWS), entre otros. En Amazon Bedrock podemos acceder a varios de estos modelos mediante una API unificada.\n",
        "\n",
        "### Rol del LLM dentro de un RAG\n",
        "\n",
        "- **Interpreta la pregunta** y detecta la intención.\n",
        "- **Lee los fragmentos recuperados** para extraer evidencia.\n",
        "- **Redacta** una respuesta final, aplicando el tono y formato especificado.\n",
        "- **Cita fuentes** si se lo pedimos explícitamente en el prompt.\n",
        "\n",
        "Sin un buen prompt, el LLM puede ignorar información relevante o inventar detalles. Por eso es fundamental controlar el mensaje que le enviamos.\n",
        "\n",
        "El LLM aporta redacción fluida, síntesis y la capacidad de adaptar el tono al público. Nuestro trabajo consiste en suministrarle el contexto correcto y un *prompt* claro para reducir al mínimo las alucinaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l6hGp5p3P17F",
      "metadata": {
        "id": "l6hGp5p3P17F"
      },
      "source": [
        "## API `retrieve_and_generate`\n",
        "\n",
        "- Combina en una sola llamada la búsqueda en la knowledge base y la generación de texto.\n",
        "- Permite indicar qué modelo usar (`modelArn`), cómo recuperar los fragmentos y cómo estructurar la generación (parámetros y *prompt template*).\n",
        "- Devuelve tanto la respuesta generada como los fragmentos utilizados, para auditar y mostrar citas.\n",
        "\n",
        "Trabajaremos directamente con esta API usando `boto3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ojeCK6HyP52s",
      "metadata": {
        "id": "ojeCK6HyP52s"
      },
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8RDhLpa5P17G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RDhLpa5P17G",
        "outputId": "5053b285-6e08-4a46-b895-f86009eb29ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cliente de Bedrock inicializados correctamente.\n"
          ]
        }
      ],
      "source": [
        "import boto3\n",
        "from typing import Any, Dict\n",
        "\n",
        "AWS_REGION = \"us-west-2\"\n",
        "KNOWLEDGE_BASE_ID = \"7DUKWTRFX3\"\n",
        "MODEL_ARN = \"us.deepseek.r1-v1:0\"\n",
        "\n",
        "# Completar con las credenciales\n",
        "session = boto3.Session(\n",
        "aws_access_key_id='',\n",
        "aws_secret_access_key='',\n",
        "aws_session_token=''\n",
        ")\n",
        "\n",
        "cliente = session.client(\n",
        "    \"bedrock-agent-runtime\",\n",
        "    region_name=AWS_REGION\n",
        ")\n",
        "\n",
        "print(\"Cliente de Bedrock inicializados correctamente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IXvcG0DNP17G",
      "metadata": {
        "id": "IXvcG0DNP17G"
      },
      "source": [
        "## Diseño del prompt\n",
        "\n",
        "Un *prompt template* típico para RAG incluye:\n",
        "- Un mensaje al modelo sobre su rol (`Eres un asistente experto en ...`).\n",
        "- Las instrucciones sobre cómo usar el contexto (`Solo responde usando la información proporcionada`).\n",
        "- El espacio reservado para insertar los fragmentos (`{context}`) y la pregunta (`{query}`).\n",
        "- Indicaciones de formato y tono.\n",
        "\n",
        "La actividad práctica se centrará en modificar este template para observar cómo cambia la respuesta."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r48eQMTKP17G",
      "metadata": {
        "id": "r48eQMTKP17G"
      },
      "source": [
        "## Implementando el flujo completo de RAG\n",
        "\n",
        "Para combinar recuperación y generación, necesitamos definir un **prompt template** que guíe al LLM sobre cómo usar los fragmentos recuperados. Este template actúa como un \"manual de instrucciones\" que le dice al modelo:\n",
        "\n",
        "- **Su rol y contexto**: qué tipo de asistente es y para qué propósito\n",
        "- **Cómo usar el contexto**: que debe basarse únicamente en los fragmentos recuperados\n",
        "- **Qué hacer cuando falta información**: cómo manejar casos donde no hay contexto suficiente\n",
        "- **Formato y tono**: cómo estructurar la respuesta\n",
        "\n",
        "El template utiliza placeholders como `$query$` y `$search_results$` que Bedrock reemplaza automáticamente con la pregunta del usuario y los fragmentos recuperados de la knowledge base.\n",
        "\n",
        "La función `generar_con_prompt()` encapsula toda la lógica técnica para invocar `retrieve_and_generate`:\n",
        "\n",
        "1. **Configura la recuperación**: especifica cuántos fragmentos recuperar (`top_k`) mediante búsqueda vectorial\n",
        "2. **Configura la generación**: define parámetros como `max_tokens` (límite de longitud) y `temperature` (control de aleatoriedad)\n",
        "3. **Construye la solicitud**: ensambla la configuración completa con el knowledge base ID, modelo ARN y el prompt template\n",
        "4. **Invoca la API**: realiza la llamada a `retrieve_and_generate` que ejecuta ambos pasos (recuperación y generación) en una sola operación\n",
        "\n",
        "El resultado incluye tanto el texto generado como las citas que vinculan cada parte de la respuesta con los fragmentos fuente, permitiendo verificar la trazabilidad y confiabilidad de la información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "P9zGAT6yP17G",
      "metadata": {
        "id": "P9zGAT6yP17G"
      },
      "outputs": [],
      "source": [
        "DEFAULT_PROMPT_TEMPLATE = \"\"\"Eres un asistente educativo especializado en Retrieval-Augmented Generation (RAG) y sistemas de búsqueda semántica.\n",
        "Tu rol es ayudar a los estudiantes a comprender conceptos relacionados con embeddings, búsqueda vectorial, chunking, similitud coseno y arquitecturas RAG.\n",
        "\n",
        "Responde la pregunta del usuario usando ÚNICAMENTE la información de los resultados de búsqueda proporcionados del material del taller.\n",
        "\n",
        "IMPORTANTE:\n",
        "- Usa ÚNICAMENTE información de los resultados de búsqueda del material educativo\n",
        "- Si no hay información suficiente en los resultados, responde: \"No encontré una respuesta exacta en el material del taller disponible\"\n",
        "- No inventes ni fabriques información que no esté presente en los resultados\n",
        "- Explica los conceptos de manera clara y educativa, ayudando a los estudiantes a entender los fundamentos de RAG\n",
        "\n",
        "Pregunta del usuario:\n",
        "\n",
        "$query$\n",
        "\n",
        "Resultados de búsqueda:\n",
        "\n",
        "$search_results$\n",
        "\n",
        "$output_format_instructions$\"\"\"\n",
        "\n",
        "\n",
        "def generar_con_prompt(\n",
        "    pregunta: str,\n",
        "    prompt_template: str = None,\n",
        "    top_k: int = 4,\n",
        "    max_tokens: int = 600,\n",
        "    temperature: float = 0.2\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Genera una respuesta usando retrieve_and_generate de Bedrock.\n",
        "\n",
        "    Args:\n",
        "        pregunta: La pregunta del usuario\n",
        "        prompt_template: Template del prompt (opcional, usa DEFAULT_PROMPT_TEMPLATE si no se proporciona)\n",
        "        top_k: Número de resultados a recuperar\n",
        "        max_tokens: Máximo de tokens en la respuesta\n",
        "        temperature: Controla la aleatoriedad/creatividad de la generación (0.0-1.0).\n",
        "                     Valores bajos (0.1-0.3) producen respuestas más deterministas y precisas,\n",
        "                     ideales para tareas que requieren exactitud. Valores altos (0.7-1.0) generan\n",
        "                     respuestas más creativas y variadas. Para RAG educativo, valores bajos (0.2)\n",
        "                     son recomendados para mantener precisión y coherencia con el contexto recuperado.\n",
        "\n",
        "    Returns:\n",
        "        Diccionario con la respuesta de la API\n",
        "    \"\"\"\n",
        "    # Usar template por defecto si no se proporciona uno\n",
        "    if prompt_template is None:\n",
        "        prompt_template = DEFAULT_PROMPT_TEMPLATE\n",
        "\n",
        "    # Configuración de recuperación\n",
        "    retrieval_config = {\n",
        "        \"vectorSearchConfiguration\": {\n",
        "            \"numberOfResults\": top_k\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Configuración base\n",
        "    config = {\n",
        "        \"type\": \"KNOWLEDGE_BASE\",\n",
        "        \"knowledgeBaseConfiguration\": {\n",
        "            \"knowledgeBaseId\": KNOWLEDGE_BASE_ID,\n",
        "            \"modelArn\": MODEL_ARN,\n",
        "            \"retrievalConfiguration\": retrieval_config,\n",
        "            \"generationConfiguration\": {\n",
        "                \"inferenceConfig\": {\n",
        "                    \"textInferenceConfig\": {\n",
        "                        \"maxTokens\": max_tokens,\n",
        "                        \"temperature\": temperature\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Parámetros de la llamada\n",
        "    params = {\n",
        "        \"input\": {\n",
        "            \"text\": pregunta\n",
        "        },\n",
        "        \"retrieveAndGenerateConfiguration\": config\n",
        "    }\n",
        "\n",
        "    # Realizar llamada a la API\n",
        "    respuesta = cliente.retrieve_and_generate(**params)\n",
        "    return respuesta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0cffd91",
      "metadata": {},
      "source": [
        "## Visualizando respuestas y trazabilidad de fuentes\n",
        "\n",
        "Una vez que el modelo genera una respuesta usando `retrieve_and_generate`, necesitamos una forma de presentar tanto el contenido generado como su trazabilidad. La función `mostrar_generacion()` procesa la respuesta estructurada de la API para extraer y formatear esta información.\n",
        "\n",
        "**Estructura de la respuesta de la API:**\n",
        "\n",
        "La respuesta de `retrieve_and_generate` contiene:\n",
        "- **`output.text`**: El texto completo generado por el modelo\n",
        "- **`citations`**: Una lista de citas que vinculan partes específicas del texto generado con sus fuentes\n",
        "\n",
        "Cada cita en `citations` tiene dos componentes principales:\n",
        "- **`generatedResponsePart`**: Contiene el fragmento de texto generado que está siendo citado, junto con su posición exacta (`span`) en el texto completo (índices `start` y `end`)\n",
        "- **`retrievedReferences`**: Una lista de referencias a los documentos fuente que respaldan esa parte de la respuesta. Cada referencia incluye:\n",
        "  - La ubicación del documento (URI de S3)\n",
        "  - Metadatos con información adicional sobre el origen\n",
        "  - Un preview del contenido del fragmento fuente\n",
        "\n",
        "**Procesamiento y visualización:**\n",
        "\n",
        "La función `mostrar_generacion()` realiza tres tareas principales:\n",
        "\n",
        "1. **Extrae y formatea el texto generado**: Presenta la respuesta completa con un ancho máximo de línea para facilitar la lectura\n",
        "2. **Procesa las citas**: Para cada cita, muestra qué parte del texto está respaldada por fuentes y sus posiciones exactas\n",
        "3. **Muestra las referencias**: Lista todas las fuentes documentales asociadas a cada cita, incluyendo un preview del contenido para verificar la relevancia\n",
        "\n",
        "Esta visualización es fundamental para **auditar la calidad del RAG**: permite verificar que el modelo está usando correctamente el contexto recuperado, identificar qué fuentes respaldan cada afirmación, y detectar posibles alucinaciones cuando partes de la respuesta no tienen referencias asociadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hZDKObSpP17H",
      "metadata": {
        "id": "hZDKObSpP17H"
      },
      "outputs": [],
      "source": [
        "from textwrap import wrap\n",
        "import json\n",
        "\n",
        "def mostrar_generacion(respuesta: Dict[str, Any]) -> None:\n",
        "    output = respuesta.get(\"output\", {})\n",
        "    texto = output.get(\"text\", \"<Sin respuesta>\")\n",
        "\n",
        "    print(\"Respuesta generada:\\n\")\n",
        "    for linea in wrap(texto, width=100):\n",
        "        print(linea)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"Citas encontradas:\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    citas = respuesta.get(\"citations\", [])\n",
        "    if not citas:\n",
        "        print(\"- No se devolvieron citas. Revisa el prompt o la coverage de la knowledge base.\")\n",
        "    else:\n",
        "        texto_completo = texto  # Guardamos el texto completo para mostrar los spans\n",
        "\n",
        "        for idx, cita in enumerate(citas, start=1):\n",
        "            # Obtener la parte del texto citada\n",
        "            generated_part = cita.get(\"generatedResponsePart\", {})\n",
        "            text_part = generated_part.get(\"textResponsePart\", {})\n",
        "            texto_citado = text_part.get(\"text\", \"\")\n",
        "            span = text_part.get(\"span\", {})\n",
        "            start = span.get(\"start\", 0)\n",
        "            end = span.get(\"end\", len(texto_completo))\n",
        "\n",
        "            print(f\"\\n--- Cita #{idx} ---\")\n",
        "            print(f\"Texto citado (posiciones {start}-{end}):\")\n",
        "            # Mostrar el texto citado con un ancho limitado\n",
        "            for linea in wrap(texto_citado, width=90):\n",
        "                print(f\"  {linea}\")\n",
        "\n",
        "            # Obtener las referencias recuperadas\n",
        "            retrieved_refs = cita.get(\"retrievedReferences\", [])\n",
        "\n",
        "            if not retrieved_refs:\n",
        "                print(\"  Referencias: Ninguna referencia recuperada para esta cita.\")\n",
        "            else:\n",
        "                print(f\"  Referencias ({len(retrieved_refs)}):\")\n",
        "                for ref_idx, ref in enumerate(retrieved_refs, start=1):\n",
        "                    # Intentar obtener la URI desde location o metadata\n",
        "                    location = ref.get(\"location\", {})\n",
        "                    metadata = ref.get(\"metadata\", {})\n",
        "\n",
        "                    # Priorizar metadata, luego location\n",
        "                    fuente = (\n",
        "                        metadata.get(\"x-amz-bedrock-kb-source-uri\") or\n",
        "                        location.get(\"s3Location\", {}).get(\"uri\") or\n",
        "                        \"Fuente desconocida\"\n",
        "                    )\n",
        "\n",
        "                    print(f\"    [{ref_idx}] {fuente}\")\n",
        "\n",
        "                    # Opcional: mostrar un preview del contenido citado\n",
        "                    content = ref.get(\"content\", {})\n",
        "                    contenido_texto = content.get(\"text\", \"\")\n",
        "                    if contenido_texto:\n",
        "                        preview = contenido_texto[:150].replace(\"\\n\", \" \").strip()\n",
        "                        if len(contenido_texto) > 150:\n",
        "                            preview += \"...\"\n",
        "                        print(f\"        Preview: {preview}\")\n",
        "\n",
        "\n",
        "pregunta_inicial = \"¿Qué métodos de búsqueda se usan para comparar vectores?\"\n",
        "prompt_personalizado = DEFAULT_PROMPT_TEMPLATE\n",
        "\n",
        "respuesta_generada = generar_con_prompt(pregunta_inicial, prompt_personalizado)\n",
        "print(json.dumps(respuesta_generada, indent=2))\n",
        "mostrar_generacion(respuesta_generada)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1w4TKdaOP17H",
      "metadata": {
        "id": "1w4TKdaOP17H"
      },
      "source": [
        "## Actividad · Añadir nuevos documentos y modificar parámetros\n",
        "\n",
        "Objetivo: demostrar cómo un flujo RAG puede incorporar conocimiento reciente sin reentrenar el modelo.\n",
        "\n",
        "1. Hacé una pregunta sobre un documento **que todavía no esté** en la knowledge base.\n",
        "2. Observá cómo responde el sistema (probablemente indicando que no tiene contexto suficiente).\n",
        "3. Agregá el documento al data source y lanzá una nueva ingesta.\n",
        "4. Volvé a hacer la pregunta y verifica cómo ahora el LLM puede responder basándose en la nueva información.\n",
        "5. Jugá con el prompt y los parámetros de la API y fijate como cambia la respuesta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HmZiOeWHP17H",
      "metadata": {
        "id": "HmZiOeWHP17H"
      },
      "outputs": [],
      "source": [
        "pregunta_nueva = \"¿Cómo se prepara una foaccia?\"\n",
        "prompt_para_nuevo_doc = DEFAULT_PROMPT_TEMPLATE\n",
        "\n",
        "respuesta = generar_con_prompt(pregunta_nueva, prompt_para_nuevo_doc)\n",
        "mostrar_generacion(respuesta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63oZIgrAP17H",
      "metadata": {
        "id": "63oZIgrAP17H"
      },
      "source": [
        "## Reflexión final\n",
        "\n",
        "- Un *prompt* bien redactado guía al modelo para usar únicamente la evidencia recuperada.\n",
        "- La API de RAG permite iterar rápidamente: añadir documentos y volver a preguntar sin reentrenar el modelo.\n",
        "- La trazabilidad (citas) es clave para generar confianza en el chatbot.\n",
        "\n",
        "En el siguiente bloque integraremos todo esto dentro de Chainlit para ofrecer una experiencia conversacional completa."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
